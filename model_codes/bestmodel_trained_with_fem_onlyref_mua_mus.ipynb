{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d2391-2f39-4c4f-8918-6065eee7bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE: This notebook is the first codebase for double encoder-decoder training\n",
    "\n",
    "gloss weight = 1 \n",
    "smoothed target \n",
    "\n",
    "\n",
    "ua = random.uniform(0.02, 0.06) \n",
    "us = random.uniform(4, 8)\n",
    "ch = random.choice([0, 1, 2])\n",
    "\n",
    "# scaling the optics value on the basis of the input ground truth reconstruction \n",
    "scale = (1+(random_mua/.3)*.01)\n",
    "\n",
    "if ch ==0: \n",
    "    ua_ = ua*random.uniform(1.001,1.02)*scale\n",
    "    us_ = us*random.uniform(0.98,0.9999)/scale\n",
    "elif ch == 1: \n",
    "    ua_ =  ua*random.uniform(0.98,0.9999)/scale\n",
    "    us_ = us*random.uniform(1.001,1.02)*scale\n",
    "else: \n",
    "    ua_ = ua*random.uniform(1.001,1.02)*scale \n",
    "    us_ =  us*random.uniform(1.001,1.02)*scale \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5259d9cc",
   "metadata": {},
   "source": [
    "NOTE: Weighted Loss effectiveness experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162ad746-e332-4a52-8a03-69527d945cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "from Models3 import * ## self defined \n",
    "from random_object_generator import Generate_Reconstruction_Image #, Generate_Reconstruction_Image_parallel\n",
    "from random_mask import get_random_mask_parallel, get_random_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5cd568",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJECTS = np.load('Data4\\\\objects_random2.npy')\n",
    "MASKS = np.load('Data4\\\\masks_random2.npy')\n",
    "OPTICSES = np.load('Data4\\\\optics_random2.npy')\n",
    "OPTICSES = np.concatenate([OPTICSES[:,:2], OPTICSES[:,:2]], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340afc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_Sample = OPTICSES.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0cff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_Reconstruction_Image_Fast(n = N_Sample):\n",
    "    sample_indxes = np.random.randint(0, n, size=16)\n",
    "    return OBJECTS[sample_indxes],  MASKS[sample_indxes], OPTICSES[sample_indxes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f357d7",
   "metadata": {},
   "source": [
    "## MODEL Definations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f164a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward_model = Forward_Operator() \n",
    "# x = forward_model(torch.randn(2,7, 32,32), torch.randn(2, 4)) #, torch.tensor([10,5])).shape\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21556686-23af-4438-b0af-f0f2b4e2e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse_model = Inverse_Operator() \n",
    "# pert = inverse_model(torch.randn(2,7, 32,32), torch.randn(2, 18, 14), torch.randn(2,4))#, torch.tensor([10,5])).shape\n",
    "# print(pert.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29398d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dis = Discriminator()\n",
    "# x = dis(torch.randn(16, 7, 32, 32), torch.randn(16, 18, 14), torch.randn(16, 4))\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7264bf49-1786-4e81-82e0-6c9fc4fba2ef",
   "metadata": {},
   "source": [
    "# Load the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a641bc61-f7eb-4ae0-a691-2a8e1f033f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "X10 = np.load(\"Data3/all_rad_mua8/All_measured_data_phan.npy\")\n",
    "y10 = np.load(\"Data3/all_rad_mua8/All_ground_truth_phan.npy\") #.transpose((0, 3,1,2))\n",
    "mask10 = np.load(\"Data3/all_rad_mua8/All_fine_meshes_phan.npy\")\n",
    "optics10 = np.load(\"Data3/all_rad_mua8/All_background_optics_phan.npy\")\n",
    "target10 = np.load(\"Data3/all_rad_mua8/Target_depth_radius_phan.npy\")\n",
    "\n",
    "\n",
    "X11 = np.load(\"Data3/rad.75/All_measured_data_phan.npy\")\n",
    "y11 = np.load(\"Data3/rad.75/All_ground_truth_phan.npy\") #.transpose((0, 3,1,2))\n",
    "mask11 = np.load(\"Data3/rad.75/All_fine_meshes_phan.npy\")\n",
    "optics11 = np.load(\"Data3/rad.75/All_background_optics_phan.npy\")\n",
    "target11 = np.load(\"Data3/rad.75/Target_depth_radius_phan.npy\")\n",
    "\n",
    "X12 = np.load(\"Data3/rad.9/All_measured_data_phan.npy\")\n",
    "y12 = np.load(\"Data3/rad.9/All_ground_truth_phan.npy\") #.transpose((0, 3,1,2))\n",
    "mask12 = np.load(\"Data3/rad.9/All_fine_meshes_phan.npy\")\n",
    "optics12 = np.load(\"Data3/rad.9/All_background_optics_phan.npy\")\n",
    "target12 = np.load(\"Data3/rad.9/Target_depth_radius_phan.npy\")\n",
    "\n",
    "X13 = np.load(\"Data3/rad1.05/All_measured_data_phan.npy\")\n",
    "y13 = np.load(\"Data3/rad1.05/All_ground_truth_phan.npy\") #.transpose((0, 3,1,2))\n",
    "mask13 = np.load(\"Data3/rad1.05/All_fine_meshes_phan.npy\")\n",
    "optics13 = np.load(\"Data3/rad1.05/All_background_optics_phan.npy\")\n",
    "target13 = np.load(\"Data3/rad1.05/Target_depth_radius_phan.npy\")\n",
    "\n",
    "X14 = np.load(\"Data3/rad1.2/All_measured_data_phan.npy\")\n",
    "y14 = np.load(\"Data3/rad1.2/All_ground_truth_phan.npy\") #.transpose((0, 3,1,2))\n",
    "mask14 = np.load(\"Data3/rad1.2/All_fine_meshes_phan.npy\")\n",
    "optics14 = np.load(\"Data3/rad1.2/All_background_optics_phan.npy\")\n",
    "target14 = np.load(\"Data3/rad1.2/Target_depth_radius_phan.npy\")\n",
    "\n",
    "\n",
    "X15 = np.load(\"Data3/depth3.5/All_measured_data_phan.npy\")\n",
    "y15 = np.load(\"Data3/depth3.5/All_ground_truth_phan.npy\") #.transpose((0, 3,1,2))\n",
    "mask15 = np.load(\"Data3/depth3.5/All_fine_meshes_phan.npy\")\n",
    "optics15 = np.load(\"Data3/depth3.5/All_background_optics_phan.npy\")\n",
    "target15 = np.load(\"Data3/depth3.5/Target_depth_radius_phan.npy\")\n",
    "\n",
    "\n",
    "X16 = np.load(\"Data3/depthmrad.75/All_measured_data_phan.npy\")\n",
    "y16 = np.load(\"Data3/depthmrad.75/All_ground_truth_phan.npy\") #.transpose((0, 3,1,2))\n",
    "mask16 = np.load(\"Data3/depthmrad.75/All_fine_meshes_phan.npy\")\n",
    "optics16 = np.load(\"Data3/depthmrad.75/All_background_optics_phan.npy\")\n",
    "target16 = np.load(\"Data3/depthmrad.75/Target_depth_radius_phan.npy\")\n",
    "\n",
    "X17 = np.load(\"Data3/depthmrad.9/All_measured_data_phan.npy\")\n",
    "y17 = np.load(\"Data3/depthmrad.9/All_ground_truth_phan.npy\") #.transpose((0, 3,1,2))\n",
    "mask17 = np.load(\"Data3/depthmrad.9/All_fine_meshes_phan.npy\")\n",
    "optics17 = np.load(\"Data3/depthmrad.9/All_background_optics_phan.npy\")\n",
    "target17 = np.load(\"Data3/depthmrad.9/Target_depth_radius_phan.npy\")\n",
    "\n",
    "X18 = np.load(\"Data3/depthmrad1.05/All_measured_data_phan.npy\")\n",
    "y18 = np.load(\"Data3/depthmrad1.05/All_ground_truth_phan.npy\") #.transpose((0, 3,1,2))\n",
    "mask18 = np.load(\"Data3/depthmrad1.05/All_fine_meshes_phan.npy\")\n",
    "optics18 = np.load(\"Data3/depthmrad1.05/All_background_optics_phan.npy\")\n",
    "target18 = np.load(\"Data3/depthmrad1.05/Target_depth_radius_phan.npy\")\n",
    "\n",
    "X19 = np.load(\"Data3/depthmrad1.2/All_measured_data_phan.npy\")\n",
    "y19 = np.load(\"Data3/depthmrad1.2/All_ground_truth_phan.npy\") #.transpose((0, 3,1,2))\n",
    "mask19 = np.load(\"Data3/depthmrad1.2/All_fine_meshes_phan.npy\")\n",
    "optics19 = np.load(\"Data3/depthmrad1.2/All_background_optics_phan.npy\")\n",
    "target19 = np.load(\"Data3/depthmrad1.2/Target_depth_radius_phan.npy\")\n",
    "\n",
    "\n",
    "X20 = np.load(\"Data3/depthmrad.5/All_measured_data_phan.npy\")\n",
    "y20 = np.load(\"Data3/depthmrad.5/All_ground_truth_phan.npy\") #.transpose((0, 3,1,2))\n",
    "mask20 = np.load(\"Data3/depthmrad.5/All_fine_meshes_phan.npy\")\n",
    "optics20 = np.load(\"Data3/depthmrad.5/All_background_optics_phan.npy\")\n",
    "target20 = np.load(\"Data3/depthmrad.5/Target_depth_radius_phan.npy\")\n",
    "\n",
    "\n",
    "X21 = np.load(\"Data3/rad.5/All_measured_data_phan.npy\")\n",
    "y21 = np.load(\"Data3/rad.5/All_ground_truth_phan.npy\") #.transpose((0, 3,1,2))\n",
    "mask21 = np.load(\"Data3/rad.5/All_fine_meshes_phan.npy\")\n",
    "optics21 = np.load(\"Data3/rad.5/All_background_optics_phan.npy\")\n",
    "target21 = np.load(\"Data3/rad.5/Target_depth_radius_phan.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e266d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([ X10, X11, X12, X13, X14, X15, X16, X17, X18, X19, X20, X21], axis = 0)\n",
    "y = np.concatenate([ y10, y11, y12, y13, y14, y15, y16, y17, y18, y19, y20, y21], axis = 0)\n",
    "Mask = np.concatenate([ mask10, mask11,  mask12,  mask13,  mask14, mask15, mask16, mask17, mask18, mask19, mask20, mask21], axis = 0)\n",
    "optics = np.concatenate([ optics10, optics11,  optics12,  optics13,  optics14, optics15, optics16, optics17, optics18, optics19, optics20, optics21], axis = 0)\n",
    "target = np.concatenate([ target10, target11, target12, target13, target14, target15, target16, target17, target18, target19, target20, target21], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c787e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.concatenate([ X20, X21], axis = 0)\n",
    "# y = np.concatenate([  y20, y21], axis = 0)\n",
    "# Mask = np.concatenate([  mask20, mask21], axis = 0)\n",
    "# optics = np.concatenate([  optics20, optics21], axis = 0)\n",
    "# target = np.concatenate([  target20, target21], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7909a476-f00d-4cab-8762-75acc44bcfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape) \n",
    "print(Mask.shape) #= X[:,:36, :]\n",
    "print(optics.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3587602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optics = np.concatenate([optics[:,:2], optics[:,:2]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70082706-ee15-48b3-9409-0fb9bb1b296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_size = 0.1\n",
    "random_state = 42\n",
    "X_train,  X_test, Mask_train,  Mask_test, optics_train, optics_test, tar_train, tar_test,  y_train, y_test = train_test_split(X, Mask, optics, target, y, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8751a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import convolve\n",
    "\n",
    "# Define the filter kernel\n",
    "filter_size = 7 #np.random.choice(filter_sizes)\n",
    "filter_kernel = np.ones((1, filter_size, filter_size))\n",
    "filter_kernel  /= np.sum(filter_kernel)\n",
    "\n",
    "filter_size2 = 3 #np.random.choice(filter_sizes)\n",
    "filter_kernel2 = np.ones((1, filter_size2, filter_size2))\n",
    "#filter_kernel2 /= np.sum(filter_kernel2)\n",
    "\n",
    "#space_with_object_avg = convolve(space_with_object, filter_kernel, mode='nearest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca8379f-4dae-4135-a056-2c0325c83ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "           \n",
    "class myDataset(Dataset): \n",
    "    def __init__(self, train = True): \n",
    "        self.X_train = X_train# .reshape(-1, 18, 14) \n",
    "        self.Mask_train = Mask_train \n",
    "        self.Mask_test = Mask_test \n",
    "        self.y_train = y_train# .transpose((0, 3,1,2)) \n",
    "        self.X_test = X_test #.reshape(-1, 18, 14)\n",
    "        self.y_test = y_test #.transpose((0, 3,1,2)) \n",
    "        self.train = train\n",
    "\n",
    "        self.optics_train = optics_train \n",
    "        self.optics_test = optics_test \n",
    "\n",
    "        self.tar_train = tar_train\n",
    "        self.tar_test = tar_test \n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train: \n",
    "            return len(self.X_train) \n",
    "        else: \n",
    "            return len(self.X_test) \n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            temp = self.y_train[idx] \n",
    "            #temp2 = y_train.max(axis = (1, 2), keepdims = True)\n",
    "            if self.tar_train[idx][1] > 0.4:\n",
    "                y_train_  = convolve(temp, filter_kernel, mode='nearest')\n",
    "                y_train_ = y_train_ + temp\n",
    "                y_train_ = convolve(y_train_, filter_kernel2, mode='nearest')\n",
    "                y_train_  = (y_train_ /(1e-7+y_train_.max(axis = (1, 2), keepdims = True)))* temp.max(axis = (1, 2), keepdims = True)\n",
    "\n",
    "                return torch.Tensor(self.X_train[idx]),torch.Tensor(self.Mask_train[idx]), torch.Tensor(self.optics_train[idx]), self.tar_train[idx],  torch.Tensor(y_train_)\n",
    "            else: \n",
    "                return torch.Tensor(self.X_train[idx]),torch.Tensor(self.Mask_train[idx]), torch.Tensor(self.optics_train[idx]), self.tar_train[idx],  torch.Tensor(y_train[idx])\n",
    "\n",
    "        else: \n",
    "            temp = self.y_test[idx] \n",
    "            #temp2 = y_train.max(axis = (1, 2), keepdims = True)\n",
    "            if self.tar_test[idx][1] > 0.4:\n",
    "                y_test_  = convolve(temp, filter_kernel, mode='nearest')\n",
    "                y_test_ = y_test_ + temp \n",
    "                y_test_ = convolve(y_test_, filter_kernel2, mode='nearest')\n",
    "\n",
    "                y_test_  = (y_test_ /(1e-7+y_test_.max(axis = (1, 2), keepdims = True)))* temp.max(axis = (1, 2), keepdims = True)\n",
    "\n",
    "                return torch.Tensor(self.X_test[idx]),torch.Tensor(self.Mask_test[idx]), torch.Tensor(self.optics_test[idx]), self.tar_test[idx],  torch.Tensor(y_test_)\n",
    "            else: \n",
    "                #return torch.Tensor(self.X_test[idx]),torch.Tensor(self.Mask_test[idx]), torch.Tensor(self.optics_test[idx]), self.tar_test[idx],  torch.Tensor(y_test[idx])\n",
    "                return torch.Tensor(self.X_test[idx]),torch.Tensor(self.Mask_test[idx]), torch.Tensor(self.optics_test[idx]), self.tar_test[idx], torch.Tensor(self.y_test[idx])\n",
    "      \n",
    "\n",
    "BATCH_SIZE = 16     \n",
    "device  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_dataset = myDataset() \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_dataset = myDataset(train= False) \n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb74df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_phan = np.load(\"phan/All_measured_data_phan.npy\")\n",
    "y_phan = np.load(\"phan/All_ground_truth_phan.npy\") #.transpose((0, 3,1,2))\n",
    "Mask_phan = np.load(\"phan/All_fine_meshes_phan.npy\")\n",
    "optics_phan = np.load(\"phan/All_background_optics_phan.npy\")\n",
    "target_phan = np.load(\"phan/Target_depth_radius_phan.npy\")\n",
    "\n",
    "optics_phan = np.concatenate([optics_phan[:,:2], optics_phan[:,:2]], axis = 1)\n",
    "\n",
    "\n",
    "class myDataset_phan(Dataset): \n",
    "    def __init__(self, train = True): \n",
    "        self.X_train = X_phan# .reshape(-1, 18, 14) \n",
    "        self.Mask_train = Mask_phan \n",
    "        self.y_train = y_phan# .transpose((0, 3,1,2)) \n",
    "        self.train = train\n",
    "\n",
    "        self.optics_train = optics_phan\n",
    "        self.tar_train = target_phan\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train: \n",
    "            return len(self.X_train) \n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            temp = self.y_train[idx] \n",
    "            if self.tar_train[idx][1] > 0.4:\n",
    "                y_train_  = convolve(temp, filter_kernel, mode='nearest')\n",
    "                y_train_ = y_train_ + temp\n",
    "                y_train_ = convolve(y_train_, filter_kernel2, mode='nearest')\n",
    "                y_train_  = (y_train_ /(1e-7+y_train_.max(axis = (1, 2), keepdims = True)))* temp.max(axis = (1, 2), keepdims = True)\n",
    "\n",
    "                return torch.Tensor(self.X_train[idx]),torch.Tensor(self.Mask_train[idx]), torch.Tensor(self.optics_train[idx]), self.tar_train[idx],  torch.Tensor(y_train_)\n",
    "            else: \n",
    "                return torch.Tensor(self.X_train[idx]),torch.Tensor(self.Mask_train[idx]), torch.Tensor(self.optics_train[idx]), self.tar_train[idx],  torch.Tensor(y_train[idx])\n",
    "            \n",
    "test_phan_dataset = myDataset_phan() \n",
    "phan_dataloader = DataLoader(test_phan_dataset, batch_size = 1, shuffle = False, drop_last = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad8f128",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e9d1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(phan_dataloader))\n",
    "mask = sample[1]\n",
    "image = sample[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f84f2a5-63e9-426d-be25-95f5434b888c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = next(iter(phan_dataloader))\n",
    "mask = sample[1]\n",
    "image = sample[4]\n",
    "\n",
    "depth = [0.5, 1.0, 1.5, 2.0, 2.5, 3, 3.5]\n",
    "def show_tensor_image2(img):\n",
    "    img = img.detach().to('cpu').numpy()\n",
    "    num_slices = 7\n",
    "    slice_positions = np.linspace(0, 6, num_slices, dtype=int)\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, num_slices, figsize=(15, 5))\n",
    "\n",
    "    # Plot slices\n",
    "    for i, pos in enumerate(slice_positions):\n",
    "        if img.shape[0]>1:\n",
    "            temp = img.squeeze()[0, i, :,:].squeeze()\n",
    "        else: \n",
    "            temp = img.squeeze()[i, :,:].squeeze()\n",
    "        \n",
    "        slice_data = temp\n",
    "        im = axes[i].imshow(slice_data,vmin = 0, vmax = 0.32, cmap='jet')\n",
    "        axes[i].set_title(f\"Depth {depth[pos]} cm\")\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "    cbar = fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.6)\n",
    "    cbar.set_label('Intensity')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "show_tensor_image2(image)\n",
    "show_tensor_image2(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb95c151-2adf-48b7-863b-9dde3c5e63b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_plot_image_all(model):\n",
    "    for step, (batchmeasured, batch_mask,b_opt, b_tar, batch_actual_image) in enumerate(test_dataloader):\n",
    "        batchmeasured = batchmeasured.to(device) #.view(-1, 18, 14) \n",
    "        batch_actual_image = batch_actual_image.to(device) #.view(-1, 7, 32, 32) \n",
    "        batch_mask = batch_mask.to(device)\n",
    "        b_opt = b_opt.to(device)\n",
    "        pred_reconstruction = model(batch_mask, batchmeasured, b_opt)\n",
    "\n",
    "        mask_ran = get_random_mask(b_tar.numpy(), batch_actual_image.cpu().numpy())\n",
    "        mask_ran = torch.tensor(mask_ran, dtype = torch.float32, device = device)\n",
    "        #mask_ran = mask_ran* mask.max(dim = (2, 3))\n",
    "\n",
    "        show_tensor_image2(batch_actual_image)\n",
    "        show_tensor_image2(mask_ran)\n",
    "        show_tensor_image2(pred_reconstruction.detach())\n",
    "\n",
    "def sample_plot_image(model):\n",
    "    for step, (batchmeasured, batch_mask, b_opt, b_tar, batch_actual_image) in enumerate(test_dataloader):\n",
    "        batchmeasured = batchmeasured.to(device) #.view(-1, 18, 14) \n",
    "        batch_actual_image = batch_actual_image.to(device) #.view(-1, 7, 32, 32) \n",
    "        batch_mask = batch_mask.to(device)\n",
    "        b_opt = b_opt.to(device)\n",
    "        pred_reconstruction = model(batch_mask, batchmeasured, b_opt)\n",
    "        mask_ran = get_random_mask(b_tar.numpy(), batch_actual_image.cpu().numpy())\n",
    "        \n",
    "        mask_ran = torch.tensor(mask_ran, dtype = torch.float32, device = device)\n",
    "        #mask_ran = mask_ran* mask.max(dim = (2, 3))\n",
    "        \n",
    "        show_tensor_image2(batch_actual_image)\n",
    "        show_tensor_image2(mask_ran)\n",
    "        show_tensor_image2(pred_reconstruction.detach())\n",
    "        break \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea494f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def error_plot(predictions, corresponding_ground_truth):\n",
    "    predictions = np.array(predictions)\n",
    "    corresponding_ground_truth = np.round(np.array(corresponding_ground_truth), 3)  # Round to 3 decimal places\n",
    "    #ground_truth = np.round(np.arange(0.12, 0.31, 0.015), 3)  # Round to 3 decimal places\n",
    "    ground_truth = np.sort(np.unique(corresponding_ground_truth))\n",
    "\n",
    "    data_dict = {value: [] for value in ground_truth}\n",
    "\n",
    "    for gt_value in ground_truth:\n",
    "        cond1 = corresponding_ground_truth > (gt_value - 0.001)\n",
    "        cond2 = corresponding_ground_truth < (gt_value + 0.001)\n",
    "        cond = cond1 & cond2\n",
    "        pred_for_value = predictions[cond]\n",
    "        if len(pred_for_value) > 0:\n",
    "            data_dict[gt_value].extend(pred_for_value)\n",
    "    \n",
    "    # Prepare data for boxplot\n",
    "    boxplot_data = [data_dict[gt_value] for gt_value in ground_truth if len(data_dict[gt_value]) > 0]\n",
    "    filtered_ground_truth = [gt_value for gt_value in ground_truth if len(data_dict[gt_value]) > 0]\n",
    "    \n",
    "    # Plotting the data\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    box = plt.boxplot(boxplot_data, positions=filtered_ground_truth, widths=0.005, patch_artist=True, showfliers=False)\n",
    "    \n",
    "    # Customize boxplot colors\n",
    "    colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99']\n",
    "    for patch, color in zip(box['boxes'], colors*int(np.ceil(len(box['boxes'])/len(colors)))):\n",
    "        patch.set_facecolor(color)\n",
    "    \n",
    "    # Customize other boxplot elements\n",
    "    for whisker in box['whiskers']:\n",
    "        whisker.set(color='#7570b3', linewidth=1.5)\n",
    "    \n",
    "    for cap in box['caps']:\n",
    "        cap.set(color='#7570b3', linewidth=1.5)\n",
    "    \n",
    "    for median in box['medians']:\n",
    "        median.set(color='red', linewidth=2)\n",
    "    \n",
    "    for flier in box['fliers']:\n",
    "        flier.set(marker='o', color='#e7298a', alpha=0.5)\n",
    "    \n",
    "    #plt.scatter(corresponding_ground_truth, predictions, color='r', alpha=0.6, s=10)\n",
    "    plt.scatter(corresponding_ground_truth, predictions, color='r', alpha=0.6, s=10)\n",
    "\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Ground Truth Values')\n",
    "    plt.ylabel('Prediction Values')\n",
    "    plt.title('Boxplot of Predictions vs Ground Truth Values')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Set x-axis limits\n",
    "    plt.xlim(0.08, 0.32)\n",
    "\n",
    "    # Rotate x-axis labels and format them\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    # Set x-axis ticks and labels\n",
    "    plt.gca().set_xticks(filtered_ground_truth)\n",
    "    plt.gca().set_xticklabels([f'{val:.3f}' for val in filtered_ground_truth])\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example data\n",
    "ground_truth = np.arange(0.12, 0.31, 0.015)\n",
    "predictions = np.random.random(100)  # Example predictions\n",
    "corresponding_ground_truth = np.random.choice(ground_truth, 100)  # Example corresponding ground truth values\n",
    "\n",
    "error_plot(predictions, corresponding_ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cab569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def mse_error(x, x_head, mask):\n",
    "    # Compute the maximum value over the last two dimensions sequentially\n",
    "    x_max = x.max(dim=-1, keepdim=False)[0].max(dim=-1, keepdim=False)[0]\n",
    "    x_head = x_head*mask \n",
    "    x_head_max = x_head.max(dim=-1, keepdim=False)[0].max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "    x_max_all7 = x.max()\n",
    "    x_head_max_all7 = x_head.max()\n",
    "\n",
    "    return F.mse_loss(x_max, x_head_max), x_max_all7.item(), x_head_max_all7.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da71da53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot_pert(pred_pert, optics_gen):\n",
    "    # plt.scatter(All_measured_data[i][:9], All_measured_data[i][9:])\n",
    "    # plt.show()\n",
    "\n",
    "    # Create a figure and an axis\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Create and add the circle to the axis\n",
    "    circle = plt.Circle((0, 0), 1, edgecolor='b', facecolor='none')\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "    # # Plot the scatter data\n",
    "    # for data in All_measured_data:\n",
    "    #     ax.scatter(data[0], data[1])\n",
    "    pred_per = pred_pert[0].detach().cpu().numpy()\n",
    "\n",
    "    # #for i in range(0,1):\n",
    "    ax.scatter(pred_per[:9],pred_per[9:] )\n",
    "    # print(\"Mua target\", All_ground_truth[i].max()) \n",
    "    print(\"Optics:\", optics_gen[0].detach().cpu().numpy())\n",
    "    # print(\"Depth, Radius\", Target_depth_radius[i])\n",
    "\n",
    "    # Set the aspect of the plot to be equal\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Set limits to ensure the circle is centered and visible\n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "\n",
    "    # Add grid for better visualization\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eb9c15-4255-4154-9dd5-d06539018dd1",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3b407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "inverse_model = Inverse_Operator().to(device)\n",
    "inverse_model.load_state_dict(torch.load('normal_checkpoints/checkpoints_epoch150/model_weights_inverse_operator_best.pth')) \n",
    "forward_model = Forward_Operator().to(device)\n",
    "forward_model.load_state_dict(torch.load('normal_checkpoints/checkpoints_epoch150/model_weights_forward_operator_best.pth')) \n",
    "dis = Discriminator(input_shape=7520).to(device)\n",
    "dis.load_state_dict(torch.load('normal_checkpoints/checkpoints_epoch150/model_weights_discriminator_best.pth')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa429380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# inverse_model = Inverse_Operator().to(device)\n",
    "# forward_model = Forward_Operator().to(device)\n",
    "# dis = Discriminator().to(device)\n",
    "opt_inverse = optim.Adam(inverse_model.parameters(), lr = 0.00002, betas = (0.5, 0.999))\n",
    "opt_forward = optim.Adam(forward_model.parameters(), lr = 0.00002, betas = (0.5, 0.999))\n",
    "opt_dis = optim.Adam(dis.parameters(), lr = 0.00002, betas = (0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b530b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[mask<1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce192f8",
   "metadata": {},
   "source": [
    "### Loss functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855742cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def Weighted_Loss(x, x_head, mask):\n",
    "    # Compute the maximum value over the last two dimensions sequentially\n",
    "    x_max = x.max(dim=-1, keepdim=True)[0].max(dim=-2, keepdim=True)[0]\n",
    "    \n",
    "    # Normalize the input tensors by the maximum value\n",
    "    x_normalized = x / (x_max + 1e-6) * mask \n",
    "    x_head_normalized = x_head / (x_max + 1e-6)* mask \n",
    "\n",
    "    #mask_ind = mask<0.01\n",
    "\n",
    "    # Compute and return the L1 loss\n",
    "    return F.l1_loss(x, x_head) + F.l1_loss(x_normalized, x_head_normalized) #  + F.l1_loss(x_head[mask_ind],mask[mask_ind])\n",
    "\n",
    "# Example usage\n",
    "loss = Weighted_Loss(torch.randn(2, 7, 32, 32), torch.randn(2, 7, 32, 32), torch.randn(2, 7, 32, 32))\n",
    "print(loss)\n",
    "\n",
    "\n",
    "bce = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f5a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fn2(inverse_model, loader, epoch=0): \n",
    "    #inverse_model.eval()\n",
    "    pbart = tqdm.tqdm(loader, leave = True)\n",
    "    mselosses = 0 \n",
    "    gt_max = []\n",
    "    pred_max = []\n",
    "\n",
    "    for idx,(pert_label, mask, b_optics, b_tar, Mua_Ground_Truth) in enumerate(pbart):\n",
    "        pert_label = pert_label.to(device) # input tabular type \n",
    "        mask = mask.to(device) # target  image type \n",
    "        b_optics = b_optics.to(device) \n",
    "        Mua_Ground_Truth = Mua_Ground_Truth.to(device)\n",
    "\n",
    "        #mask_ran = get_random_mask(b_tar.numpy(), Mua_Ground_Truth.cpu().numpy())\n",
    "        #mask_ran = torch.tensor(mask_ran, dtype = torch.float32, device = device)\n",
    "        mask_ran = mask \n",
    "\n",
    "        ## train perturbation 2 perturbation model \n",
    "        ##########################################\n",
    "        Mua_Recons = inverse_model(mask_ran, pert_label, b_optics)\n",
    "        mse, gt, pred = mse_error(Mua_Ground_Truth, Mua_Recons, mask_ran)\n",
    "\n",
    "        gt_max.append(gt)\n",
    "        pred_max.append(pred)\n",
    "\n",
    "        mselosses += mse.item()\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            pbart.set_postfix(\n",
    "                Mean_Square_Error = mse.item(), \n",
    "            )\n",
    "    #inverse_model.train()\n",
    "    # plt.scatter(gt_max, pred_max)\n",
    "    # plt.show()\n",
    "\n",
    "    if epoch%3 ==0: \n",
    "        error_plot(pred_max, gt_max)\n",
    "        \n",
    "    return mselosses/len(pbart) , r2_score(gt_max, pred_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74266321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fn(inverse_model, forward_model, dis, loader, opt_inverse, opt_forward, opt_dis, epoch):\n",
    "    #inverse_model.eval()\n",
    "    pbart = tqdm.tqdm(loader, leave = True)\n",
    "    mselosses = 0 \n",
    "    gt_max = []\n",
    "    pred_max = []\n",
    "\n",
    "    for idx,(pert_label, mask, b_optics, b_tar, Mua_Ground_Truth) in enumerate(pbart):\n",
    "        pert_label = pert_label.to(device) # input tabular type \n",
    "        mask = mask.to(device) # target  image type \n",
    "        b_optics = b_optics.to(device) \n",
    "        Mua_Ground_Truth = Mua_Ground_Truth.to(device)\n",
    "\n",
    "        #mask_ran = get_random_mask(b_tar.numpy(), Mua_Ground_Truth.cpu().numpy())\n",
    "        #mask_ran = torch.tensor(mask_ran, dtype = torch.float32, device = device)\n",
    "        mask_ran = mask \n",
    "\n",
    "        ## train perturbation 2 perturbation model \n",
    "        ##########################################\n",
    "        Mua_Recons = inverse_model(mask, pert_label, b_optics)\n",
    "        mse, gt, pred = mse_error(Mua_Ground_Truth, Mua_Recons, mask_ran)\n",
    "\n",
    "        gt_max.append(gt)\n",
    "        pred_max.append(pred)\n",
    "\n",
    "        mselosses += mse.item()\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            pbart.set_postfix(\n",
    "                Mean_Square_Error = mse.item(), \n",
    "            )\n",
    "    #inverse_model.train()\n",
    "    # plt.scatter(gt_max, pred_max)\n",
    "    # plt.show()\n",
    "\n",
    "    if epoch%3 ==0: \n",
    "        error_plot(pred_max, gt_max)\n",
    "        \n",
    "    return mselosses/len(pbart) , r2_score(gt_max, pred_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8f78d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(inverse_model, forward_model, dis, loader, opt_inverse, opt_forward, opt_dis, epoch):\n",
    "    pbar = tqdm.tqdm(loader, leave = True)\n",
    "    #print(len(pbar))\n",
    "    #for idx,(batchmeasured, batch_mask, batch_actual_image) in enumerate(pbar):\n",
    "    for idx,(pert_label, mask, b_optics, b_tar, Mua_Ground_Truth) in enumerate(pbar):\n",
    "        pert_label = pert_label.to(device) # input tabular type \n",
    "        mask = mask.to(device) # target  image type \n",
    "        mask_ran = mask \n",
    "        #mask = mask.max(axis = 1)\n",
    "        b_optics = b_optics.to(device) \n",
    "        Mua_Ground_Truth = Mua_Ground_Truth.to(device)\n",
    "\n",
    "        #mask_ran = get_random_mask(b_tar.numpy(), Mua_Ground_Truth.cpu().numpy())\n",
    "        #mask_ran = torch.tensor(mask_ran, dtype = torch.float32, device = device)\n",
    "\n",
    "\n",
    "        ## train perturbation 2 perturbation model \n",
    "        ##########################################\n",
    "        Mua_Recons = inverse_model(mask_ran, pert_label, b_optics)\n",
    "        pert_pred = forward_model(Mua_Recons, b_optics)\n",
    "        loss_pert = F.l1_loss(pert_label[:,:18,:], pert_pred)\n",
    "        loss_recons1 = Weighted_Loss(Mua_Ground_Truth*mask_ran, Mua_Recons, mask_ran)*1.5 #* 0.1\n",
    "        #loss_recons2 = loss_pert # F.mse_loss(Mua_Ground_Truth.view(BATCH_SIZE, 7, -1).max(dim = -1)[0], Mua_Recons.view(BATCH_SIZE, 7, -1).max(dim = -1)[0])*0.1\n",
    "        loss_recons = loss_recons1 + loss_pert #loss_recons2\n",
    "\n",
    "        \n",
    "\n",
    "        ### trian the discriminator \n",
    "        #pert_pred = forward_model(Mua_Recons, b_optics)\n",
    "        D_real = dis(mask_ran, pert_label, torch.cat([b_optics[:,:2].repeat([1,20]), Mua_Ground_Truth.amax(dim = [1,2,3]).unsqueeze(1).repeat([1,60])], axis =1))\n",
    "        D_real_loss = bce(D_real, torch.ones_like(D_real))\n",
    "\n",
    "        pert_pred_noisy = pert_pred + torch.randn_like(pert_pred)*0.25\n",
    "        D_fake = dis(mask_ran, pert_pred_noisy.detach(), torch.cat([b_optics[:,:2].repeat([1,20]), Mua_Ground_Truth.amax(dim = [1,2,3]).unsqueeze(1).repeat([1,60])], axis =1))\n",
    "        D_fake_loss = bce(D_fake, torch.zeros_like(D_fake))\n",
    "\n",
    "\n",
    "        # Weight update for the inverse operator \n",
    "        opt_inverse.zero_grad()\n",
    "        opt_forward.zero_grad()\n",
    "        loss_recons.backward()\n",
    "        opt_inverse.step()\n",
    "        opt_forward.step()\n",
    "\n",
    "\n",
    "        ## train reconstruction 2 reconstruction model \n",
    "        ##########################################\n",
    "        Mua_GT_generated, mask_gen, optics_gen = Generate_Reconstruction_Image_Fast()\n",
    "        Mua_GT_generated = torch.tensor(Mua_GT_generated, dtype = torch.float32, device = device) \n",
    "        mask_gen = torch.tensor(mask_gen, dtype = torch.float32, device = device)\n",
    "        optics_gen = torch.tensor(optics_gen, dtype = torch.float32, device = device)\n",
    "        pert_pred2 = forward_model(Mua_GT_generated, optics_gen)\n",
    "\n",
    "        ### trian the discriminator\n",
    "\n",
    "        pert_pred2_noisy = pert_pred2 + torch.randn_like(pert_pred2)*0.25\n",
    "        D_fake2 = dis(mask_gen, pert_pred2_noisy.detach(), torch.cat([optics_gen[:,:2].repeat([1,20]), Mua_GT_generated.amax(dim = [1,2,3]).unsqueeze(1).repeat([1,60])], axis =1))\n",
    "        D_fake_loss2 = bce(D_fake2, torch.zeros_like(D_fake))\n",
    "\n",
    "        # Weight update for the discriminator \n",
    "        D_loss = (D_fake_loss*0.2 + D_real_loss + D_fake_loss2)/3\n",
    "        opt_dis.zero_grad()\n",
    "        D_loss.backward() #retain_graph = True)\n",
    "        opt_dis.step()\n",
    "        #continue\n",
    "        \n",
    "        \n",
    "        \n",
    "        Mua_Recons = inverse_model(mask_gen, pert_pred2, optics_gen)\n",
    "        D_fake3 = dis(mask_gen, pert_pred2 , torch.cat([optics_gen[:,:2].repeat(1,20), Mua_GT_generated.amax(dim = [1,2,3]).unsqueeze(1).repeat([1,60])], axis =1))\n",
    "        G_fake_loss = bce(D_fake3, torch.ones_like(D_fake))\n",
    "        \n",
    "        loss_recons3 = Weighted_Loss(Mua_GT_generated*mask_gen, Mua_Recons, mask_gen)*1.5 #*0.1\n",
    "        #loss_recons4 = 0 #F.mse_loss(Mua_GT_generated.view(BATCH_SIZE, 7, -1).max(dim = -1)[0], Mua_Recons.view(BATCH_SIZE, 7, -1).max(dim = -1)[0])*0.1\n",
    "\n",
    "        loss_recons5 = loss_recons3 +  G_fake_loss*0.001\n",
    "\n",
    "        # Weight update for the inverse operator \n",
    "        opt_inverse.zero_grad()\n",
    "        opt_forward.zero_grad()\n",
    "        loss_recons5.backward()\n",
    "        opt_inverse.step()\n",
    "        opt_forward.step()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        if epoch % 3 == 0 and idx == 0:\n",
    "            print(\"r2r\")\n",
    "            show_tensor_image2(Mua_GT_generated)\n",
    "            show_tensor_image2(mask_gen)\n",
    "            show_tensor_image2(Mua_Recons)\n",
    "            scatter_plot_pert(pert_pred, b_optics)\n",
    "            scatter_plot_pert(pert_pred2, optics_gen)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "        if idx % 10 == 0:\n",
    "            pbar.set_postfix(\n",
    "                P2P_loss_pert = loss_pert.item(),\n",
    "                P2P_loss_recons = loss_recons1.item(),\n",
    "                R2R_loss_recons = loss_recons3.item(),\n",
    "                D_fake = D_fake_loss.item(),\n",
    "                D_fake2 = D_fake_loss2.item(),\n",
    "                D_real = D_real_loss.item(), \n",
    "                R2R_G_fake_loss = G_fake_loss.item()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fd8498",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "best_r_score = -10 \n",
    "best_phan_score = -10\n",
    "for epoch in range(epochs): \n",
    "    train_fn(inverse_model, forward_model, dis, train_dataloader, opt_inverse, opt_forward, opt_dis, epoch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        metrics = test_fn(inverse_model, forward_model, dis, test_dataloader, opt_inverse, opt_forward, opt_dis, epoch)\n",
    "\n",
    "        \n",
    "        if best_r_score < metrics[1]:\n",
    "                torch.save(inverse_model.state_dict(), 'checkpoints/model_weights_inverse_operator_best.pth')\n",
    "                torch.save(forward_model.state_dict(), 'checkpoints/model_weights_forward_operator_best.pth')\n",
    "                torch.save(dis.state_dict(), 'checkpoints/model_weights_discriminator_best.pth')\n",
    "\n",
    "                best_r_score = metrics[1]\n",
    "                \n",
    "        if epoch% 3==0: \n",
    "            #print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()}\") \n",
    "            metrics_phan = test_fn2(inverse_model, phan_dataloader)\n",
    "\n",
    "            if best_phan_score < metrics_phan[1]:\n",
    "                best_phan_score = metrics_phan[1]\n",
    "\n",
    "                torch.save(inverse_model.state_dict(), 'checkpoints/phan/model_weights_inverse_operator_best.pth')\n",
    "                torch.save(forward_model.state_dict(), 'checkpoints/phan/model_weights_forward_operator_best.pth')\n",
    "                torch.save(dis.state_dict(), 'checkpoints/phan/model_weights_discriminator_best.pth')\n",
    "\n",
    "            print(\"P2P Inverse Model:: \", \"Epoch: \", epoch, \"Test Mse: \", metrics[0],\"Test r_score: \", metrics[1], \"Test phan r_score: \", metrics_phan[1])\n",
    "            \n",
    "            sample_plot_image(inverse_model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e920fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_r_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ff3c1a-fd67-4500-aa4c-2522064dad2e",
   "metadata": {},
   "source": [
    "# Testing Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7310d0ac-bb58-4fd9-8ae6-50d6329e00ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_plot_image_all(inverse_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81f397b-0b32-482a-8d68-9487cb2edaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(inverse_model.state_dict(), 'checkpoints/model_weights_inverse_operator.pth')\n",
    "torch.save(forward_model.state_dict(), 'checkpoints/model_weights_forward_operator.pth')\n",
    "torch.save(dis.state_dict(), 'checkpoints/model_weights_discriminator.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba8d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_model = Inverse_Operator()  # Initialize the model\n",
    "inverse_model.load_state_dict(torch.load('checkpoints/experiment1/model_weights_inverse_operator.pth'))  # Load the weights\n",
    "inverse_model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424414e2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iqbal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
